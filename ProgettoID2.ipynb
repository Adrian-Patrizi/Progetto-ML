{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZX1TmBFPx-1a",
        "CtAPGJiqyBMW",
        "_4Vnn0ogyJ-0",
        "3KUNh79uyO3S",
        "VhDoMP9VyTY3"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Progetto Machine Learning ID2"
      ],
      "metadata": {
        "id": "CIiblj1xx6PC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Studente:** Adrian Patrizi\n",
        "\n",
        "**Matricola:** 2094287\n",
        "\n",
        "**Email:** patrizi.2094287@studenti.uniroma1.it\n",
        "\n",
        "**Progetto scelto:** ID 2: Audio Restoration for Generative Models — Improving MusicGen Outputs\n"
      ],
      "metadata": {
        "id": "SXqIA64U9f6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "ZX1TmBFPx-1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import json\n",
        "import warnings\n",
        "import zipfile\n",
        "import gdown\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "pPobUDmn3KEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "hvd7B9z389V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download audio\n"
      ],
      "metadata": {
        "id": "dkzRfydJ3hZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://drive.google.com/file/d/1TDkcfnqOSVib4i2RwdyB1bIJ18RetLhB/view?usp=sharing\"\n",
        "output = \"/content/checkpoint.zip\"\n",
        "\n",
        "if not os.path.exists(output):\n",
        "    gdown.download(id=\"1TDkcfnqOSVib4i2RwdyB1bIJ18RetLhB\", output=output, quiet=False)\n",
        "\n",
        "with zipfile.ZipFile(output, \"r\") as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "print(\"Cartella checkpoint pronta\")"
      ],
      "metadata": {
        "id": "tmD_4tEOb9Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scarica FMA Small - 8GB\n",
        "!wget https://os.unil.cloud.switch.ch/fma/fma_small.zip\n",
        "!unzip fma_small.zip"
      ],
      "metadata": {
        "id": "V6SZ16dq8_jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In questa cella vengono selezionati e suddivisi i file MP3 del dataset FMA Small in tre insiemi: train, validation e test.\n",
        "\n",
        "Tramite `glob.glob('./fma_small/*/*.mp3')` estraiamo tutti i file `.mp3` nelle cartelle, poi con `random.shuffle(files)` viene mescolato l'elenco ed infine selezioniamo i primi 1000.\n",
        "\n",
        "I file vengono poi suddivisi in:\n",
        "   - 800 per il training,  \n",
        "   - 100 per la validazione,  \n",
        "   - 100 per il test.\n",
        "\n"
      ],
      "metadata": {
        "id": "xezD9Pgg3bRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepara i file\n",
        "random.seed(101)\n",
        "\n",
        "files = glob.glob('./fma_small/*/*.mp3')\n",
        "random.shuffle(files)\n",
        "files = files[:1000]\n",
        "\n",
        "train_files = files[:800]\n",
        "val_files = files[800:900]\n",
        "test_files = files[900:]\n",
        "print(len(train_files), len(val_files), len(test_files))"
      ],
      "metadata": {
        "id": "7uC0YnC69GD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "UR2pHqMNx_8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Classe AudioDataset"
      ],
      "metadata": {
        "id": "vdpAlCqc9i7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questa classe definisce il dataset personalizzato per l’addestramento del modello di audio enhancement.  \n",
        "Il suo obiettivo è simulare degradazioni realistiche, generando al volo (on-the-fly) coppie degraded - clean da cui la rete apprende a ricostruire il segnale originale.\n",
        "\n",
        "**Funzionalità principali:**\n",
        "\n",
        "1. **Parametri principali**\n",
        "   - `audio_files`: lista dei percorsi dei file audio.  \n",
        "   - `sample_rate`: frequenza di campionamento (32 kHz).  \n",
        "   - `segment_length`: durata del segmento estratto in secondi.  \n",
        "   - `degradation_types`: tipi di degradazioni applicate (quantizzazione, resample, low-pass, clipping).  \n",
        "   - `identity_prob`: probabilità che un esempio resti non degradato (≈ 15%).\n",
        "   - `seed`: opzionale, per riproducibilità.\n",
        "\n",
        "2. **Metodo `degrade_audio()`**\n",
        "   Applica degradazioni audio:\n",
        "   - **Quantizzazione:** riduce la profondità in bit con dithering casuale.  \n",
        "   - **Low-pass:** filtra le alte frequenze con una soglia casuale (3.5-8 kHz).  \n",
        "   - **Resample:** durante il downsampling, le alte frequenze oltre metà del nuovo sample rate vengono eliminate completamente.  \n",
        "   - **Clipping:** limita l’ampiezza per introdurre distorsione armonica.  \n",
        "   In caso di errore nella compressione, aggiunge un leggero rumore gaussiano.\n",
        "\n",
        "3. **Metodo `__getitem__()`**\n",
        "   - Carica e segmenta il file audio in un frammento di durata fissa.  \n",
        "   - Normalizza il segnale in ampiezza.  \n",
        "   - Applica la degradazione (oppure mantiene l’identità con probabilità 0.15).  \n",
        "   - Calcola i **mel-spettrogrammi** pulito e degradato (`n_mels=128`, `n_fft=1024`, `hop=256`).  \n",
        "   - Converte in scala logaritmica (dB) e normalizza in **[0, 1]**, compatibile con l’output della rete.  \n",
        "   - Restituisce un dizionario contenente:\n",
        "     - `'degraded'`: spettrogramma degradato (input del modello)  \n",
        "     - `'clean'`: spettrogramma pulito (target)  \n",
        "     - `'degraded_audio'` e `'clean_audio'`: forme d’onda corrispondenti per eventuali analisi extra."
      ],
      "metadata": {
        "id": "e-fzLsc-4WX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTA:\n",
        "librosa.power_to_db() converte da potenza → decibel (logaritmo in base 10)\n",
        "\n",
        "$$\n",
        "\\text{mel}_{\\text{dB}} = 10 \\cdot \\log_{10}\\left( \\frac{\\text{mel}}{\\text{ref}} \\right)\n",
        "$$\n",
        "\n",
        "\n",
        "`ref = np.max` imposta il valore massimo del mel-spettrogramma a 0 dB, cioè la banda più intensa ha valore 0 tutte le altre saranno valori negativi da -1 a -80 dB\n"
      ],
      "metadata": {
        "id": "HiZH-ItA_yHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(Dataset):\n",
        "\n",
        "    def __init__(self, audio_files, sample_rate=32000, segment_length=3.0,\n",
        "                 degradation=['quantize', 'resample', 'reverb', 'clipping'],\n",
        "                 identity_prob=0.15, seed=None):\n",
        "\n",
        "        self.audio_files = audio_files\n",
        "        self.sr = sample_rate\n",
        "        self.segment_samples = int(segment_length * sample_rate)\n",
        "        self.degradation = degradation\n",
        "        self.identity_prob = identity_prob\n",
        "        self.seed = seed\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "\n",
        "    def degrade_audio(self, audio):\n",
        "\n",
        "        x = audio.copy()\n",
        "\n",
        "        # 1) Quantizzazione bit-depth + dithering\n",
        "        if 'quantize' in self.degradation:\n",
        "            bits = np.random.choice([8, 10, 12])\n",
        "            q = 2 ** bits\n",
        "            dither = np.random.uniform(-0.5/q, 0.5/q, size=x.shape)\n",
        "            x = np.round(x * q) / q + dither\n",
        "            x = np.clip(x, -1.0, 1.0)\n",
        "\n",
        "        # 2) Downsampling (simula compressione e perdita di banda)\n",
        "        if 'resample' in self.degradation:\n",
        "            new_sr = np.random.choice([16000, 22050])\n",
        "            y = librosa.resample(x, orig_sr=self.sr, target_sr=new_sr)\n",
        "            x = librosa.resample(y, orig_sr=new_sr, target_sr=self.sr)\n",
        "\n",
        "            # riallinea lunghezza\n",
        "            if len(x) > len(audio):\n",
        "                x = x[:len(audio)]\n",
        "            elif len(x) < len(audio):\n",
        "                x = np.pad(x, (0, len(audio) - len(x)))\n",
        "\n",
        "        # 3) Riverbero artificiale (coda esponenziale)\n",
        "        if 'reverb' in self.degradation:\n",
        "            reverb_ms = np.random.uniform(50, 200)  # lunghezza in ms\n",
        "            reverb_samples = int(self.sr * reverb_ms / 1000)\n",
        "            ir = np.exp(-np.linspace(0, 5, reverb_samples))  # impulso decrescente\n",
        "            x = np.convolve(x, ir, mode='full')[:len(x)]\n",
        "            x = x / (np.max(np.abs(x)) + 1e-9)\n",
        "\n",
        "        # 4) Clipping leggero\n",
        "        if 'clipping' in self.degradation:\n",
        "            clip_thr = float(np.random.uniform(0.8, 0.95))\n",
        "            x = np.clip(x, -clip_thr, clip_thr)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed + idx)\n",
        "\n",
        "        # Caricamento e segmentazione\n",
        "        audio, sr = librosa.load(self.audio_files[idx], sr=self.sr, mono=True)\n",
        "        if len(audio) > self.segment_samples:\n",
        "            start = np.random.randint(0, len(audio) - self.segment_samples)\n",
        "            audio = audio[start:start + self.segment_samples]\n",
        "        else:\n",
        "            audio = np.pad(audio, (0, self.segment_samples - len(audio)))\n",
        "\n",
        "        # Normalizzazione\n",
        "        max_val = np.max(np.abs(audio)) + 1e-8\n",
        "        audio = audio / max_val\n",
        "\n",
        "        # Identità stocastica: 15% dei campioni non degradati\n",
        "        if np.random.rand() < self.identity_prob:\n",
        "            degraded = audio.copy()\n",
        "        else:\n",
        "            degraded = self.degrade_audio(audio)\n",
        "\n",
        "        # Mel-spectrogrammi\n",
        "        clean_mel = librosa.feature.melspectrogram(\n",
        "            y=audio, sr=self.sr, n_mels=128, n_fft=1024, hop_length=256\n",
        "        )\n",
        "        degraded_mel = librosa.feature.melspectrogram(\n",
        "            y=degraded, sr=self.sr, n_mels=128, n_fft=1024, hop_length=256\n",
        "        )\n",
        "\n",
        "        # Conversione in dB (log10) + normalizzazione [0,1]\n",
        "        clean_mel = librosa.power_to_db(clean_mel, ref=np.max, top_db=80.0)\n",
        "        degraded_mel = librosa.power_to_db(degraded_mel, ref=np.max, top_db=80.0)\n",
        "        clean_mel = (clean_mel + 80) / 80\n",
        "        degraded_mel = (degraded_mel + 80) / 80\n",
        "\n",
        "        return {\n",
        "            'degraded': torch.FloatTensor(degraded_mel).unsqueeze(0),\n",
        "            'clean': torch.FloatTensor(clean_mel).unsqueeze(0),\n",
        "            'degraded_audio': torch.FloatTensor(degraded),\n",
        "            'clean_audio': torch.FloatTensor(audio)\n",
        "        }"
      ],
      "metadata": {
        "id": "oUhB_1Rb9JOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Istanziazione del dataset"
      ],
      "metadata": {
        "id": "f32-FIx_9pHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = AudioDataset(\n",
        "    train_files,\n",
        "    sample_rate=32000,\n",
        "    segment_length=4.0\n",
        ")\n",
        "\n",
        "\n",
        "val_dataset = AudioDataset(\n",
        "    val_files,\n",
        "    sample_rate=32000,\n",
        "    segment_length=4.0,\n",
        "    seed = 99\n",
        ")\n",
        "\n",
        "\n",
        "test_dataset = AudioDataset(\n",
        "    test_files,\n",
        "    sample_rate=32000,\n",
        "    segment_length=4.0,\n",
        "    seed = 201\n",
        ")"
      ],
      "metadata": {
        "id": "VNhVEPwG9WJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "-D0kV9Pu4MCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizzazione esempio"
      ],
      "metadata": {
        "id": "_kIBOUaX-VVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Qui verifichiamo visivamente e acusticamente la correttezza del dataset generato.\n",
        "\n",
        "Per farlo selezioniamo un indice (es. `idx = 0`) dal `train_dataset`. Si estrae un esempio contenente sia il segnale pulito che quello degradato.\n",
        "\n",
        "   - `clean_audio` e `degraded_audio` sono le forme d’onda originali in formato NumPy.\n",
        "   - `clean_mel` e `degraded_mel` sono i corrispondenti mel-spettrogrammi normalizzati (128 bande x T frame).\n",
        "\n",
        "Come prima cosa viene riprodotto il segnale pulito e quello degradato direttamente in notebook per valutare a orecchio la qualità della degradazione.\n",
        "\n",
        "Successivamete vengono mostrati i grafici dei mel-spettrogrammi in scala logaritmica:\n",
        "     - **Clean Mel-Spectrogram:** rappresenta il riferimento ad alta qualità.\n",
        "     - **Degraded Mel-Spectrogram:** mostra l’effetto delle degradazioni introdotte.\n"
      ],
      "metadata": {
        "id": "TRG4wuQt5WMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prendiamo un indice\n",
        "idx = 44\n",
        "\n",
        "# Estrai l'elemento dal dataset\n",
        "sample = train_dataset[idx]\n",
        "\n",
        "# --- Estrai dati ---\n",
        "clean_audio = sample['clean_audio'].numpy()\n",
        "degraded_audio = sample['degraded_audio'].numpy()\n",
        "\n",
        "clean_mel = sample['clean'].squeeze().numpy()       # [128, T]\n",
        "degraded_mel = sample['degraded'].squeeze().numpy() # [128, T]\n",
        "\n",
        "print(f\"Audio length (samples): {len(clean_audio)}\")\n",
        "\n",
        "# --- Audio ---\n",
        "print(\"\\n Clean audio:\")\n",
        "display(Audio(clean_audio, rate=32000))\n",
        "\n",
        "print(\"Degraded audio:\")\n",
        "display(Audio(degraded_audio, rate=32000))\n",
        "\n",
        "# --- Spettrogrammi ---\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "img1 = axes[0].imshow(clean_mel, aspect='auto', origin='lower', cmap='magma')\n",
        "axes[0].set_title('Clean Mel-Spectrogram')\n",
        "axes[0].set_ylabel('Mel Frequency')\n",
        "fig.colorbar(img1, ax=axes[0])\n",
        "\n",
        "img2 = axes[1].imshow(degraded_mel, aspect='auto', origin='lower', cmap='magma')\n",
        "axes[1].set_title('Degraded Mel-Spectrogram')\n",
        "axes[1].set_ylabel('Mel Frequency')\n",
        "axes[1].set_xlabel('Time Frames')\n",
        "fig.colorbar(img2, ax=axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vyzAfKli-XKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader"
      ],
      "metadata": {
        "id": "CtAPGJiqyBMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qui creiamo i DataLoader per gli insiemi di training, validazione e test, gestendo la lettura parallela dei campioni e la riproducibilità dei risultati.\n",
        "\n",
        "La Funzione `worker_init_fn(worker_id)` serve a sincronizzare i semi casuali (seed) di ciascun worker del DataLoader, garantendo che operazioni casuali (come le degradazioni audio o l’estrazione dei segmenti) producano risultati riproducibili anche quando eseguite in più thread.\n",
        "\n",
        "\n",
        "Vengono infine istanziati i DataLoader:\n",
        "- `train_loader` che usa `shuffle=True` per mescolare i batch a ogni epoca.  \n",
        "- `val_loader` e `test_loader` che mantengono `shuffle=False` per garantire coerenza nella valutazione.  \n",
        "- `batch_size=4` imposta il numero di esempi per batch.  \n",
        "- `num_workers=2` abilita il caricamento in parallelo per migliorare la velocità.  \n",
        "- `pin_memory=True` velocizza il trasferimento dei batch alla GPU."
      ],
      "metadata": {
        "id": "2qrJzVYI7PVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def worker_init_fn(worker_id):\n",
        "\n",
        "    # Ottieni il seed globale di PyTorch per questo worker\n",
        "    # % 2**32 lo converte in un intero compatibile con NumPy\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n"
      ],
      "metadata": {
        "id": "UUSm6Ry97Q3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    worker_init_fn=worker_init_fn\n",
        ")\n",
        "\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    worker_init_fn=worker_init_fn\n",
        ")\n",
        "\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    worker_init_fn=worker_init_fn\n",
        ")"
      ],
      "metadata": {
        "id": "IES0qu8n9V5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modello"
      ],
      "metadata": {
        "id": "3doua2pAyEFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questa architettura combina reti convoluzionali (CNN) e ricorrenti (RNN) per migliorare la qualità degli spettrogrammi audio degradati.\n",
        "\n",
        "L’obiettivo è prendere un Mel-spectrogramma degradato di forma `[B, 1, 128, T]`  e produrre un Mel-spectrogramma potenziato della stessa dimensione.\n",
        "\n",
        "**Struttura**\n",
        "\n",
        "- Encoder (3 blocchi CNN + ResidualBlock):\n",
        "\n",
        "  estrae rappresentazioni locali di frequenza e tempo, riducendo gradualmente la risoluzione temporale (stride=2).\n",
        "\n",
        "- RNN bottleneck (GRU bidirezionale):\n",
        "\n",
        "  cattura dipendenze temporali globali, modellando la coerenza lungo l’asse del tempo. Opera su feature flattenate per ogni frame temporale.\n",
        "\n",
        "- Decoder (3 blocchi ConvTranspose + skip connections):\n",
        "\n",
        "  ricostruisce il Mel potenziato combinando le feature temporali e spaziali.  \n",
        "  Gli skip connection (`torch.cat`) riutilizzano le informazioni dell’encoder (U-Net style).\n",
        "\n",
        "- Residual Mode:\n",
        "  se attivo (`residual_mode=True`), il modello predice un residuo da sommare all’input, invece di rigenerare il Mel completo.\n",
        "\n",
        "**Output finale**\n",
        "Il tensore in uscita è vincolato in `[0,1]` tramite Sigmoid, coerente con i Mel normalizzati.  Se `residual_mode=True`, il risultato è la somma tra input e residuo, con `clamp` per mantenere il range valido.\n"
      ],
      "metadata": {
        "id": "tOXsTHRAKXdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(dropout),\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        out = out + x\n",
        "        return F.relu(out)\n",
        "\n",
        "\n",
        "class AudioEnhancementCRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_mels=128, hidden_channels=[24, 48, 96],\n",
        "                 rnn_hidden=128, rnn_layers=1, dropout=0.2,\n",
        "                 residual_mode=True):\n",
        "        super().__init__()\n",
        "        self.n_mels = n_mels\n",
        "        self.residual_mode = residual_mode\n",
        "\n",
        "        # ----- Encoder -----\n",
        "        # Input: [B, 1, 128, T]\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Conv2d(1, hidden_channels[0], 5, padding=2), # --> [B, 24, 128, T]\n",
        "            nn.BatchNorm2d(hidden_channels[0]),\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(hidden_channels[0], dropout=dropout)\n",
        "        )\n",
        "\n",
        "        self.enc2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_channels[0], hidden_channels[1], 4, stride=2, padding=1), # --> [B, 48, 64, T/2]\n",
        "            nn.BatchNorm2d(hidden_channels[1]),\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(hidden_channels[1], dropout=dropout)\n",
        "        )\n",
        "\n",
        "        self.enc3 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_channels[1], hidden_channels[2], 4, stride=2, padding=1), # --> [B, 96, 32, T/4]\n",
        "            nn.BatchNorm2d(hidden_channels[2]),\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(hidden_channels[2], dropout=dropout)\n",
        "        )\n",
        "\n",
        "        # ----- RNN bottleneck -----\n",
        "        # Appiattisce le feature su (T/4) e modella la dinamica temporale globale\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=hidden_channels[2] * (n_mels // 4), #ogni frame e 96x32 features\n",
        "            hidden_size=rnn_hidden,\n",
        "            num_layers=rnn_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if rnn_layers > 1 else 0.0\n",
        "        )\n",
        "        self.rnn_projection = nn.Linear(rnn_hidden * 2, hidden_channels[2] * (n_mels // 4))\n",
        "        # Output: stessa dimensione delle feature dell'encoder (per skip connections)\n",
        "\n",
        "        # ----- Decoder -----\n",
        "        # Decodifica simmetrica in stile U-Net con concatenazione skip connections\n",
        "        self.dec3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(hidden_channels[2] * 2, hidden_channels[1], 4, stride=2, padding=1), # --> [B, 48, 64, T/2]\n",
        "            nn.BatchNorm2d(hidden_channels[1]),\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(hidden_channels[1], dropout=dropout)\n",
        "        )\n",
        "\n",
        "        self.dec2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(hidden_channels[1] * 2, hidden_channels[0], 4, stride=2, padding=1), # --> [B, 24, 128, T]\n",
        "            nn.BatchNorm2d(hidden_channels[0]),\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(hidden_channels[0], dropout=dropout)\n",
        "        )\n",
        "\n",
        "        self.dec1 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_channels[0] * 2, hidden_channels[0], 3, padding=1), # --> [B, 24, 128, T]\n",
        "            nn.BatchNorm2d(hidden_channels[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels[0], 1, 1), # --> [B, 1, 128, T]\n",
        "            nn.Sigmoid()  # vincola output in [0,1]\n",
        "        )\n",
        "\n",
        "    def crop_match(self, a, b):\n",
        "        min_t = min(a.size(-1), b.size(-1))\n",
        "        return a[..., :min_t], b[..., :min_t]\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ----- Encoder -----\n",
        "        e1 = self.enc1(x)       # [B, 24, 128, T]\n",
        "        e2 = self.enc2(e1)      # [B, 48, 64, T/2]\n",
        "        e3 = self.enc3(e2)      # [B, 96, 32, T/4]\n",
        "\n",
        "        # ----- RNN -----\n",
        "        B, C, F, T = e3.shape\n",
        "        rnn_in = e3.permute(0, 3, 1, 2).reshape(B, T, -1)     # [B, T/4, 96×32]\n",
        "        rnn_out, _ = self.rnn(rnn_in)                         # [B, T/4, 2×rnn_hidden]\n",
        "        rnn_out = self.rnn_projection(rnn_out)                # [B, T/4, 96×32]\n",
        "        rnn_out = rnn_out.reshape(B, T, C, F).permute(0, 2, 3, 1)  # [B, 96, 32, T/4]\n",
        "\n",
        "        # ----- Decoder -----\n",
        "        d3 = self.dec3(torch.cat([e3, rnn_out], dim=1))       # concat skip: [B, 192, 32, T/4] --> [B, 48, 64, T/2]\n",
        "        e2, d3 = self.crop_match(e2, d3)\n",
        "        d2 = self.dec2(torch.cat([e2, d3], dim=1))            # [B, 96, 64, T/2] --> [B, 24, 128, T]\n",
        "        e1, d2 = self.crop_match(e1, d2)\n",
        "        d1 = self.dec1(torch.cat([e1, d2], dim=1))            # [B, 48, 128, T] --> [B, 1, 128, T]\n",
        "\n",
        "        # ----- Residual connection -----\n",
        "        if self.residual_mode:\n",
        "            if d1.size(-1) != x.size(-1):\n",
        "                min_t = min(d1.size(-1), x.size(-1))\n",
        "                d1 = d1[..., :min_t]\n",
        "                x = x[..., :min_t]\n",
        "            d1 = x + d1                                        # output = input + residuo\n",
        "\n",
        "        return torch.clamp(d1, 0.0, 1.0)                      # assicura output in [0,1]"
      ],
      "metadata": {
        "id": "sUC-QcYT-SO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "exGRk-0fyGJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questa classe combina più termini di loss per valutare quanto il Mel potenziato\n",
        "si avvicina al target pulito, bilanciando fedeltà globale, precisione sulle alte frequenze.\n",
        "\n",
        "**Componenti principali**\n",
        "1. Charbonnier loss (`charb`)\n",
        "   Variante della L1-loss che penalizza meno i grandi errori e stabilizza il training. Formula:  \n",
        "   $$ L_{charb} = \\sqrt{(x - y)^2 + \\varepsilon} $$\n",
        "\n",
        "2. High-Frequency weighted loss (`hf`)\n",
        "   Pesa di più le bande Mel alte (quelle sopra ~6 kHz), per spingere il modello a ricostruire meglio brillantezza e dettagli armonici persi con la degradazione. Formula:\n",
        "   $$\n",
        "   \\mathcal{L}_{\\text{hf}} =\n",
        "   \\frac{1}{B C M T} \\sum_{b,c,m,t}\n",
        "   w_m \\, \\big| x_{b,c,m,t} - y_{b,c,m,t} \\big|\n",
        "   $$\n",
        "   con pesi lineari definiti come:\n",
        "   $$\n",
        "   w_m = 0.5 + \\frac{m}{M-1}\n",
        "   $$\n",
        "   in modo che $ w_m $ cresca da 0.5 (basse frequenze) a 1.5 (alte frequenze).\n",
        "\n",
        "**Loss totale**\n",
        "$$L_{tot} = w_{charb} \\cdot L_{charb} + w_{hf} \\cdot L_{hf}$$\n",
        "\n",
        "I pesi di default (`w_charb=1.0, w_hf=0.3`) bilanciano i contributi.\n"
      ],
      "metadata": {
        "id": "nbGnDrWlRUOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioRestorationLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, n_fft=1024, hop_length=256,\n",
        "                 w_charb=1.0, w_hf=0.3, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.w_charb = w_charb\n",
        "        self.w_hf = w_hf\n",
        "        self.eps = eps\n",
        "\n",
        "    def charbonnier_loss(self, x, y):\n",
        "        diff = x - y\n",
        "        return torch.mean(torch.sqrt(diff * diff + self.eps))\n",
        "\n",
        "    def hf_weighted_loss(self, x, y):\n",
        "\n",
        "        # Estrazione delle dimensioni\n",
        "        B, C, M, T = x.shape  # [B,1,n_mels,time]\n",
        "\n",
        "        # Creazione del vettore dei pesi w_m\n",
        "        # linspace genera vettore 1D di m pesi tra 0.5 e 1.5\n",
        "        # con view lo ridimensionamento per broadcasting sul batch [B, 1, M, T]\n",
        "        w = torch.linspace(0.5, 1.5, M, device=x.device).view(1, 1, M, 1)\n",
        "        return torch.mean(w * torch.abs(x - y))\n",
        "\n",
        "    def forward(self, pred_mel, target_mel, residual_mode=False):\n",
        "        # === Mel losses ===\n",
        "        charb = self.charbonnier_loss(pred_mel, target_mel)\n",
        "        hf = self.hf_weighted_loss(pred_mel, target_mel)\n",
        "\n",
        "        total_loss = (self.w_charb * charb + self.w_hf * hf)\n",
        "\n",
        "        metrics = {\n",
        "            'charb': charb.item(),\n",
        "            'hf': hf.item(),\n",
        "            'mode': 'residual' if residual_mode else 'direct'\n",
        "        }\n",
        "\n",
        "        return total_loss, metrics"
      ],
      "metadata": {
        "id": "NhW6KZ1v_XDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "_4Vnn0ogyJ-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "3KUNh79uyO3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questa funzione gestisce l’intero processo di addestramento  del modello CRNN,  \n",
        "includendo resume da checkpoint, validazione, salvataggio automatico e early stopping.\n",
        "\n",
        "**Flusso principale**\n",
        "1. Preparazione\n",
        "   - Attiva ottimizzazioni CUDA (`cudnn.benchmark`) per massimizzare le prestazioni.  \n",
        "   - Crea una directory di salvataggio separata per la modalità corrente (`residual` o `direct`).\n",
        "\n",
        "2. Inizializzazione\n",
        "   - Ottimizzatore: `AdamW`, con `weight_decay` per regolarizzazione.  \n",
        "   - Scheduler: `ReduceLROnPlateau`, dimezza il learning rate se la validazione non migliora per 5 epoche.  \n",
        "   - Loss: `AudioRestorationLoss()` (combinazione Charbonnier + HF).\n",
        "\n",
        "3. Resume\n",
        "   - Se `resume=True`, ricarica modello, ottimizzatore, scheduler e cronologia dal checkpoint precedente.\n",
        "\n",
        "4. Training Loop\n",
        "   - Per ogni epoca:\n",
        "     - imposta il modello in modalità `train()`, azzera i gradienti,  \n",
        "       calcola la loss batch per batch e aggiorna i pesi.\n",
        "     - Clip dei gradienti (max_norm=1.0) per evitare esplosioni numeriche.\n",
        "     - Calcola media delle loss di training.\n",
        "\n",
        "5. Validazione\n",
        "   - Imposta `model.eval()` e disattiva il gradiente (`torch.no_grad()`).\n",
        "   - Calcola la loss media sui dati di validazione per monitorare il progresso.\n",
        "\n",
        "6. Gestione checkpoint\n",
        "   - Salva automaticamente il miglior modello quando la `val_loss` migliora.\n",
        "   - Se la validazione non migliora per 5 epoche consecutive, attiva early stopping.\n",
        "   - Ogni 10 epoche salva comunque un checkpoint intermedio (`checkpoint_epoch_X.pt`).\n",
        "\n",
        "7. Output\n",
        "   - Restituisce la cronologia (`history`) con le curve `train_loss` e `val_loss`.\n",
        "\n"
      ],
      "metadata": {
        "id": "DPQ_zfVqVjMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTA:\n",
        "Durante il training la loss lavora solo nel dominio Mel (`charb + hf`) poiché è molto più veloce e stabile, mentre durante la validazione viene inclusa anche la `STFT loss` che misura la somiglianza spettrale tra audio ricostruito e target, questo perché vogliamo verificare anche quanto bene il suono ricostruito\n",
        "è simile all’originale in termini di spettro."
      ],
      "metadata": {
        "id": "ujI6IV9_Xf8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs=20,\n",
        "    lr=1e-3,\n",
        "    save_dir='checkpoints',\n",
        "    resume=False,\n",
        "    resume_path=None\n",
        "):\n",
        "\n",
        "    # ---------- Ottimizzazioni GPU ----------\n",
        "    # benchmark=True --> PyTorch sceglie automaticamente la conv più veloce per la GPU\n",
        "    # deterministic=False --> leggermente più veloce ma non completamente riproducibile\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "\n",
        "    # ---------- Preparazione cartelle ----------\n",
        "    mode_name = \"residual\" if model.residual_mode else \"direct\"\n",
        "    save_dir = os.path.join(save_dir, mode_name)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # ---------- Ottimizzatore, scheduler e loss ----------\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6\n",
        "    )\n",
        "    criterion = AudioRestorationLoss()\n",
        "\n",
        "    # ---------- Resume logic ----------\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float('inf')\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    if resume:\n",
        "      ckpt_path = resume_path or os.path.join(save_dir, 'best_model.pt')\n",
        "      if os.path.exists(ckpt_path):\n",
        "          print(f\"Resuming training from checkpoint: {ckpt_path}\")\n",
        "          checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
        "\n",
        "          # Rimuovi eventuale prefisso \"_orig_mod.\"\n",
        "          state_dict = checkpoint['model_state_dict']\n",
        "          new_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
        "          model.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "          optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "          if 'scheduler_state_dict' in checkpoint:\n",
        "              scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "          start_epoch = checkpoint.get('epoch', 0) + 1\n",
        "          best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "          history = checkpoint.get('history', history)\n",
        "          print(f\" Resumed from epoch {start_epoch}, best val loss {best_val_loss:.6f}\")\n",
        "\n",
        "\n",
        "    # ---------- TRAINING LOOP ----------\n",
        "    patience_counter = 0\n",
        "    patience_limit = 5\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_components = {'charb': [], 'hf': []}\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for batch in pbar:\n",
        "            degraded = batch['degraded'].to(device)\n",
        "            clean = batch['clean'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            enhanced = model(degraded) #forward pass\n",
        "\n",
        "            # Allinea lunghezza temporale (dovuta agli stride del modello)\n",
        "            if enhanced.size(-1) != clean.size(-1):\n",
        "                min_t = min(enhanced.size(-1), clean.size(-1))\n",
        "                enhanced = enhanced[..., :min_t]\n",
        "                clean = clean[..., :min_t]\n",
        "\n",
        "            # Calcolo della loss (solo Mel domain durante il training)\n",
        "            loss, metrics = criterion(\n",
        "                enhanced, clean, residual_mode=model.residual_mode\n",
        "            )\n",
        "            loss.backward()\n",
        "\n",
        "            # Clipping dei gradienti per stabilità\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            for k in train_components.keys():\n",
        "                train_components[k].append(metrics[k])\n",
        "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "\n",
        "        # ---------- VALIDATION ----------\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "\n",
        "            pbar_val = tqdm(val_loader, desc=f\"Validation {epoch+1}/{epochs}\")\n",
        "            for batch in pbar_val:\n",
        "                degraded = batch['degraded'].to(device)\n",
        "                clean = batch['clean'].to(device)\n",
        "                enhanced = model(degraded)\n",
        "\n",
        "                if enhanced.size(-1) != clean.size(-1):\n",
        "                    min_t = min(enhanced.size(-1), clean.size(-1))\n",
        "                    enhanced = enhanced[..., :min_t]\n",
        "                    clean = clean[..., :min_t]\n",
        "\n",
        "                #Sistemare uso stft\n",
        "                loss, _ = criterion(\n",
        "                    enhanced, clean, residual_mode=model.residual_mode\n",
        "                )\n",
        "\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "\n",
        "        # ---------- Log e scheduler ----------\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}: Train={avg_train_loss:.4f} | Val={avg_val_loss:.4f}\")\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # ---------- SALVATAGGIO CHECKPOINTS ----------\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'history': history\n",
        "            }, os.path.join(save_dir, 'best_model.pt'))\n",
        "            print(\"Saved new best model\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"Patience counter: {patience_counter}\")\n",
        "\n",
        "        if patience_counter >= patience_limit:\n",
        "            print(f\"\\n Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        # Salvataggio checkpoint ogni 10 epoche\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            ckpt_name = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pt')\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'history': history\n",
        "            }, ckpt_name)\n",
        "            print(f\" Saved checkpoint: {ckpt_name}\")\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "3-DWar8g_-VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot"
      ],
      "metadata": {
        "id": "eCiWG4gTyRXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot delle curve di training e validazione"
      ],
      "metadata": {
        "id": "JU2uFeZKfM9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history, mode='direct'):\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "    ax.plot(history['train_loss'], label='Train', color='tab:blue')\n",
        "    ax.plot(history['val_loss'], label='Validation', color='tab:orange')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_title(f'Training & Validation Loss ({mode} mode)')\n",
        "    ax.legend()\n",
        "    ax.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1zE3NWwzCmER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "VhDoMP9VyTY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metriche"
      ],
      "metadata": {
        "id": "knImFvGEyau_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La Log-Spectral Distance misura quanto differiscono due spettri, in questo caso i Mel in dB. Più è piccolo il valore, più lo spettro stimato assomiglia a quello reale. Formula:\n",
        "\n",
        "$$\n",
        "\\text{LSD} =\n",
        "\\frac{1}{T} \\sum_{t=1}^{T}\n",
        "\\sqrt{\n",
        "\\frac{1}{M} \\sum_{m=1}^{M}\n",
        "\\left(\n",
        "S_{\\text{clean}}(m, t) - S_{\\text{test}}(m, t)\n",
        "\\right)^2\n",
        "}\n",
        "$$"
      ],
      "metadata": {
        "id": "XKqYeO31ggoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_lsd_db(clean_mel_db, test_mel_db):\n",
        "    eps = 1e-8\n",
        "    diff = clean_mel_db - test_mel_db\n",
        "    return float(np.mean(np.sqrt(np.mean(diff**2, axis=0) + eps)))"
      ],
      "metadata": {
        "id": "HDvcOWXGDaox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Misura la somiglianza direzionale tra due vettori in questo caso gli spettri appiattiti. I valori sono tra -1 e 1 dove:\n",
        "* 1 --> perfetta somiglianza;\n",
        "* 0 --> ortogonali;\n",
        "* -1 --> opposti.\n",
        "\n",
        "È utile per capire se il modello conserva la “forma” dello spettro anche quando l’ampiezza differisce. Formula:\n",
        "$$\n",
        "\\text{Cosine}(p, t) =\n",
        "\\frac{\n",
        "\\langle p, t \\rangle\n",
        "}{\n",
        "\\|p\\|_2 \\, \\|t\\|_2\n",
        "}\n",
        "$$"
      ],
      "metadata": {
        "id": "palpYYnkgrjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_similarity_mel(pred_mel, target_mel):\n",
        "    \"\"\"Cosine similarity tra due mel-spettrogram flatten.\"\"\"\n",
        "    # pred_mel, target_mel: numpy arrays shape (n_mels, T)\n",
        "    # appiattisci in vettori\n",
        "    p = pred_mel.flatten()\n",
        "    t = target_mel.flatten()\n",
        "    # dot / (||p|| * ||t||)\n",
        "    num = np.dot(p, t)\n",
        "    den = (np.linalg.norm(p) * np.linalg.norm(t) + 1e-12)\n",
        "    return num / den"
      ],
      "metadata": {
        "id": "ADmTk11BDRf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Test Loop"
      ],
      "metadata": {
        "id": "2zNukT54ydDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questa funzione valuta le prestazioni del modello confrontando **quanto il Mel potenziato (enhanced)**\n",
        "si avvicina al Mel pulito (clean), rispetto al Mel degradato (degraded).\n",
        "\n",
        "L’obiettivo è misurare se e quanto il modello riduce la distanza tra il segnale degradato e quello pulito.\n",
        "\n",
        "1. Per ogni clip nel `test_loader`:\n",
        "   - calcola lo spettrogramma Mel potenziato (`enhanced`) tramite il modello;\n",
        "   - confronta i Mel-spettri degraded --> clean e enhanced --> clean;\n",
        "   - valuta le differenze usando tre metriche:\n",
        "     - L1 --> errore assoluto medio (più piccolo è meglio);\n",
        "     - LSD (Log-Spectral Distance) --> distanza percettiva in dB (più piccolo è meglio);\n",
        "     - Cosine Similarity --> correlazione spettrale (più grande è meglio).\n",
        "\n",
        "2. Calcola i guadagni (gain) come differenza tra la metrica del segnale degradato e quella dell’enhanced:\n",
        "   - `gain_l1`, `gain_lsd` > 0 ⇒ l’enhanced è più vicino al clean (miglioramento);\n",
        "   - `gain_cos` > 0 ⇒ la similarità spettrale è aumentata.\n",
        "\n",
        "3. Salva tutti i risultati in un file `.csv` (`metrics_mel_improvement.csv`) e stampa le medie aggregate.\n",
        "\n",
        "**Come usare la funzione di valutazione**\n",
        "\n",
        "Se stai usando il **modello diretto** (output = Mel potenziato):\n",
        "```python\n",
        "evaluate_model(model, test_loader, device, residual_mode=False)\n",
        "```\n",
        "\n",
        "Se stai usando la **modalità residua** (output = Mel residuo sommato al degradato):\n",
        "\n",
        "```python\n",
        "evaluate_model(model, test_loader, device, residual_mode=True)\n",
        "```"
      ],
      "metadata": {
        "id": "_2a7EN1dmL4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device,\n",
        "                                   save_dir='results_mel_improvement',\n",
        "                                   residual_mode=False):\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    model.eval()\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating (Mel improvement)\"):\n",
        "            # Estrarre Mel degradato e pulito [B, 1, M, T] normalizzati in [0,1]\n",
        "            degraded = batch['degraded'].to(device)\n",
        "            clean    = batch['clean'].to(device)\n",
        "\n",
        "            # Forward pass del modello\n",
        "            enh_part = torch.clamp(model(degraded), 0.0, 1.0)\n",
        "\n",
        "            # Gestione delle due modalità operative\n",
        "            if residual_mode:\n",
        "                # Modalità residuo --> somma output + input degradato\n",
        "                T = min(enh_part.size(-1), degraded.size(-1), clean.size(-1))\n",
        "                degraded = degraded[..., :T]\n",
        "                clean    = clean[..., :T]\n",
        "                enh_part = enh_part[..., :T]\n",
        "                enhanced = torch.clamp(degraded + enh_part, 0.0, 1.0)\n",
        "            else:\n",
        "                # Modalità diretta --> l'output è già il Mel potenziato\n",
        "                T = min(enh_part.size(-1), clean.size(-1))\n",
        "                enhanced = enh_part[..., :T]\n",
        "                clean    = clean[..., :T]\n",
        "                degraded = degraded[..., :T]\n",
        "\n",
        "            # Conversione a NumPy per le metriche\n",
        "            enh_np = enhanced.cpu().squeeze(1).numpy()\n",
        "            cln_np = clean.cpu().squeeze(1).numpy()\n",
        "            deg_np = degraded.cpu().squeeze(1).numpy()\n",
        "\n",
        "            # Ciclo sui singoli esempi del batch\n",
        "            for i in range(len(cln_np)):\n",
        "                # Converti da scala [0,1] → [-80,0] dB per il calcolo della LSD\n",
        "                mel_clean_db = cln_np[i] * 80.0 - 80.0\n",
        "                mel_enh_db   =  enh_np[i] * 80.0 - 80.0\n",
        "                mel_deg_db   =  deg_np[i] * 80.0 - 80.0\n",
        "\n",
        "                # ---------- METRICHE ----------\n",
        "                # L1: errore medio assoluto in scala lineare [0,1]\n",
        "                l1_deg = np.mean(np.abs(deg_np[i] - cln_np[i]))\n",
        "                l1_enh = np.mean(np.abs(enh_np[i] - cln_np[i]))\n",
        "                gain_l1 = l1_deg - l1_enh  # >0 --> miglioramento\n",
        "\n",
        "                # LSD: distanza logaritmica percettiva in dB\n",
        "                lsd_deg = compute_lsd_db(mel_clean_db, mel_deg_db)\n",
        "                lsd_enh = compute_lsd_db(mel_clean_db, mel_enh_db)\n",
        "                gain_lsd = lsd_deg - lsd_enh\n",
        "\n",
        "                 # Cosine Similarity: coerenza nella forma spettrale\n",
        "                cos_deg = compute_cosine_similarity_mel(deg_np[i], cln_np[i])\n",
        "                cos_enh = compute_cosine_similarity_mel(enh_np[i], cln_np[i])\n",
        "                gain_cos = cos_enh - cos_deg # >0 --> miglioramento\n",
        "\n",
        "                # Salva tutte le metriche per la clip corrente\n",
        "                rows.append({\n",
        "                    # valori assoluti\n",
        "                    'l1_deg': l1_deg, 'l1_enh': l1_enh,\n",
        "                    'lsd_deg': lsd_deg, 'lsd_enh': lsd_enh,\n",
        "                    'cos_deg': cos_deg, 'cos_enh': cos_enh,\n",
        "                    # guadagni (positivi = miglioramento)\n",
        "                    'gain_l1': gain_l1,\n",
        "                    'gain_lsd': gain_lsd,\n",
        "                    'gain_cos': gain_cos\n",
        "                })\n",
        "\n",
        "    # DataFrame con tutte le metriche raccolte\n",
        "    df = pd.DataFrame(rows)\n",
        "    means = df.mean(numeric_only=True).to_dict()\n",
        "\n",
        "     # ---------- RISULTATI ----------\n",
        "    print(\"\\n=== Evaluation Results (Mel Improvement) ===\")\n",
        "    print(f\"l1_deg:  {means['l1_deg']:.4f} | l1_enh:  {means['l1_enh']:.4f} | gain_l1:  {means['gain_l1']:.4f}\")\n",
        "    print(f\"lsd_deg: {means['lsd_deg']:.4f} | lsd_enh: {means['lsd_enh']:.4f} | gain_lsd: {means['gain_lsd']:.4f}\")\n",
        "    print(f\"cos_deg: {means['cos_deg']:.4f} | cos_enh: {means['cos_enh']:.4f} | gain_cos: {means['gain_cos']:.4f}\")\n",
        "    print(\"Nota: gain_l1, gain_lsd, gain_cos > 0 indicano miglioramento\")\n",
        "\n",
        "    # Salva i risultati dettagliati in CSV\n",
        "    out_csv = os.path.join(save_dir, 'metrics_mel_improvement.csv')\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"\\nSaved detailed metrics to {out_csv}\")\n",
        "\n",
        "    return df, means\n"
      ],
      "metadata": {
        "id": "fELDFQXi-dP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizza e confronta i Mel-spettri Clean, Degraded e Enhanced per un singolo esempio permettendo una valutazione qualitativa del comportamento del modello.\n",
        "Parametri:\n",
        "  * model: rete neurale addestrata per il miglioramento audio\n",
        "  * sample: dizionario {'degraded', 'clean'} restituito dal Dataset\n",
        "  * device: 'cpu' o 'cuda'\n",
        "  * save_path: percorso dove salvare l'immagine risultante\n",
        "\n",
        "Output: immagine con i tre spettrogrammi a confronto."
      ],
      "metadata": {
        "id": "hAvW-SjH2LHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_enhancement(model, sample, device, save_path='plots/enhancement.png'):\n",
        "\n",
        "    # Assicura che la directory di salvataggio esista\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Prepara i dati per l'inferenza, aggiunge dim batch\n",
        "    degraded = sample['degraded'].unsqueeze(0).to(device)\n",
        "\n",
        "    # Estrae il Mel pulito per confronto e lo converte in numpy per il plotting\n",
        "    clean = sample['clean'].squeeze().cpu().numpy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Ottiene il Mel potenziato dal modello e converte in numpy\n",
        "        enhanced = model(degraded).cpu().squeeze().numpy()\n",
        "\n",
        "    # Estrae anche il Mel degradato originale come numpy\n",
        "    degraded = sample['degraded'].squeeze().numpy()\n",
        "\n",
        "    # ---------- VISUALIZZAZIONE ----------\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(10, 8))\n",
        "    for ax, data, title in zip(axes, [clean, degraded, enhanced],\n",
        "                               ['Clean', 'Degraded', 'Enhanced']):\n",
        "        img = ax.imshow(data, aspect='auto', origin='lower', cmap='magma')\n",
        "        ax.set_title(title)\n",
        "        fig.colorbar(img, ax=ax)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "MKSKZTzRp32g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Esperimenti"
      ],
      "metadata": {
        "id": "ibEKMR6GyhHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la variabile `no_training` si gestisce lo skip della fase di training per un'esecuzione più rapida. Di base è settata a `True`, se si vuole eseguire il training cambiare a `False`"
      ],
      "metadata": {
        "id": "azrkaX62gi7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_training = True"
      ],
      "metadata": {
        "id": "xXlWnI3AgF9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modello MEL diretta"
      ],
      "metadata": {
        "id": "ZNsZhy5ryjBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir_direct = 'checkpoints'\n",
        "os.makedirs(save_dir_direct, exist_ok=True)\n",
        "\n",
        "# Inizializza il modello\n",
        "model_direct = AudioEnhancementCRNN(residual_mode=False).to(device)\n",
        "print(f\"Model ready on {device}, trainable parameters: {sum(p.numel() for p in model_direct.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# Esegui training\n",
        "ckpt_direct = '/content/checkpoints/direct/best_model.pt'\n",
        "\n",
        "if no_training and os.path.exists(ckpt_direct):\n",
        "    print(f\" Checkpoint già esistente: {ckpt_direct}. Skip training\")\n",
        "\n",
        "elif no_training:\n",
        "    print(\"Nessun checkpoint trovato. Avvio del training\")\n",
        "    history_direct = train_model(\n",
        "        model_direct,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        epochs=20,\n",
        "        lr=1e-4,\n",
        "        save_dir=save_dir_direct,\n",
        "        resume=False\n",
        "    )\n",
        "\n",
        "    # Plot loss\n",
        "    plot_training_history(history_direct, mode='direct')\n",
        "\n",
        "else:\n",
        "    print(\"Avvio del training\")\n",
        "    history_direct = train_model(\n",
        "        model_direct,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        epochs=20,\n",
        "        lr=1e-4,\n",
        "        save_dir=save_dir_direct,\n",
        "        resume=False\n",
        "    )\n",
        "\n",
        "    # Plot loss\n",
        "    plot_training_history(history_direct, mode='direct')"
      ],
      "metadata": {
        "id": "dh8q-z_r_yLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Valutazione sul test set\n",
        "if os.path.exists(ckpt_direct):\n",
        "    # Load the checkpoint with weights_only=False\n",
        "    checkpoint = torch.load(ckpt_direct, map_location=device, weights_only=False)\n",
        "    model_direct.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
        "else:\n",
        "    print(\" Nessun checkpoint trovato\")\n",
        "\n",
        "df_direct, metrics_direct = evaluate_model(\n",
        "    model_direct,\n",
        "    test_loader,\n",
        "    device,\n",
        "    save_dir='results_direct',\n",
        "    residual_mode=False\n",
        ")"
      ],
      "metadata": {
        "id": "r0uX5DzjEQdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizzazione qualitativa\n",
        "sample_idx = np.random.randint(0, len(test_dataset))\n",
        "sample = test_dataset[sample_idx]\n",
        "visualize_enhancement(model_direct, sample, device, save_path='plots/enhancement_direct.png')"
      ],
      "metadata": {
        "id": "rXKQQn04p9QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modello su residui"
      ],
      "metadata": {
        "id": "uYQqMeF0yksn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir_residual = 'checkpoints'\n",
        "os.makedirs(save_dir_residual, exist_ok=True)\n",
        "\n",
        "# Inizializza il modello\n",
        "model_residual = AudioEnhancementCRNN(residual_mode=True).to(device)\n",
        "print(f\"Model ready on {device}, trainable parameters: {sum(p.numel() for p in model_residual.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# Esegui training\n",
        "ckpt_residual = '/content/checkpoints/residual/best_model.pt'\n",
        "if no_training and os.path.exists(ckpt_residual):\n",
        "    print(f\" Checkpoint già esistente: {ckpt_residual}. Skip training\")\n",
        "\n",
        "elif no_training:\n",
        "    print(\"Nessun checkpoint trovato. Avvio del training\")\n",
        "    history_residual = train_model(\n",
        "        model_residual,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        epochs=20,\n",
        "        lr=1e-4,\n",
        "        save_dir=save_dir_residual,\n",
        "        resume=False\n",
        "    )\n",
        "\n",
        "    # Plot loss\n",
        "    plot_training_history(history_residual, mode='residual')\n",
        "\n",
        "else:\n",
        "    history_residual = train_model(\n",
        "        model_residual,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        epochs=20,\n",
        "        lr=1e-4,\n",
        "        save_dir=save_dir_residual,\n",
        "        resume=False\n",
        "    )\n",
        "\n",
        "    # Plot loss\n",
        "    plot_training_history(history_residual, mode='residual')"
      ],
      "metadata": {
        "id": "fukdYGRf_mQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Valutazione sul test set\n",
        "if os.path.exists(ckpt_residual):\n",
        "    checkpoint = torch.load(ckpt_residual, map_location=device, weights_only=False)\n",
        "    model_residual.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
        "else:\n",
        "    print(\" Nessun checkpoint trovato\")\n",
        "\n",
        "df_residual, metrics_residual = evaluate_model(\n",
        "    model_residual,\n",
        "    test_loader,\n",
        "    device,\n",
        "    save_dir='results_residual',\n",
        "    residual_mode=False\n",
        ")"
      ],
      "metadata": {
        "id": "PMyS9fjWERbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizzazione qualitativa\n",
        "sample_idx = np.random.randint(0, len(test_dataset))\n",
        "sample = test_dataset[sample_idx]\n",
        "visualize_enhancement(model_residual, sample, device, save_path='plots/enhancement_residual.png')"
      ],
      "metadata": {
        "id": "4xmamATmqAX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confronto"
      ],
      "metadata": {
        "id": "sz6tH2TLEOGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tabella comparativa sui guadagni medi\n",
        "comparison = pd.DataFrame([\n",
        "    {\n",
        "        'gain_l1': metrics_direct['gain_l1'],\n",
        "        'gain_lsd': metrics_direct['gain_lsd'],\n",
        "        'gain_cos': metrics_direct['gain_cos']\n",
        "    },\n",
        "    {\n",
        "        'gain_l1': metrics_residual['gain_l1'],\n",
        "        'gain_lsd': metrics_residual['gain_lsd'],\n",
        "        'gain_cos': metrics_residual['gain_cos']\n",
        "    }\n",
        "], index=['Direct', 'Residual']).T\n",
        "\n",
        "print(\"\\n=== Confronto finale dei guadagni (Mel-domain) ===\")\n",
        "display(comparison.round(4))\n",
        "\n",
        "# ---- Plot dei guadagni medi ----\n",
        "ax = comparison.plot.bar(\n",
        "    figsize=(10,5),\n",
        "    colormap='viridis',\n",
        "    edgecolor='black'\n",
        ")\n",
        "plt.title('Confronto tra modello diretto e residuo (guadagno medio)')\n",
        "plt.ylabel('Δ rispetto al degraded (positivo = miglioramento)')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cCfldyyqEU83"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}